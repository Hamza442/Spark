{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c8ed0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6938f1bd",
   "metadata": {},
   "source": [
    "# Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68ea734f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Java', '20000'), ('Python', '100000'), ('Scala', '3000')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "           \n",
    "# create a rdd \n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "614c1253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|    _1|    _2|\n",
      "+------+------+\n",
      "|  Java| 20000|\n",
      "|Python|100000|\n",
      "| Scala|  3000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD1 = rdd.toDF()\n",
    "dfFromRDD1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2be1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca6fb400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD1=rdd.toDF(columns)\n",
    "dfFromRDD1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec09dfbd",
   "metadata": {},
   "source": [
    "# createDataFrame from SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d45cfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD2 = spark.createDataFrame(data).toDF(*columns)\n",
    "\n",
    "dfFromRDD2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc2561",
   "metadata": {},
   "source": [
    "# Create DataFrame with schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b26e132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "\n",
    "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\",StringType(),True),\n",
    "    StructField(\"middlename\",StringType(),True),\n",
    "    StructField(\"lastname\",StringType(),True),\n",
    "    StructField(\"id\",StringType(),True),\n",
    "    StructField(\"gender\",StringType(),True),\n",
    "    StructField(\"salary\",StringType(),True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=data2,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75021756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19bd05dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dea30a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10051464",
   "metadata": {},
   "source": [
    "# DataFrame from Data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dad39b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----------+-------------------+-----+--------------+-----+------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-----+\n",
      "|RecordNumber|Zipcode|ZipCodeType|               City|State|  LocationType|  Lat|  Long|Xaxis|Yaxis|Zaxis|WorldRegion|Country|        LocationText|            Location|Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|Notes|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-----+\n",
      "|           1|    704|   STANDARD|        PARC PARQUE|   PR|NOT ACCEPTABLE|17.96|-66.22| 0.38|-0.87|  0.3|         NA|     US|     Parc Parque, PR|NA-US-PR-PARC PARQUE|        FALSE|           null|               null|      null| null|\n",
      "|           2|    704|   STANDARD|PASEO COSTA DEL SUR|   PR|NOT ACCEPTABLE|17.96|-66.22| 0.38|-0.87|  0.3|         NA|     US|Paseo Costa Del S...|NA-US-PR-PASEO CO...|        FALSE|           null|               null|      null| null|\n",
      "|          10|    709|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14|-66.26| 0.38|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        FALSE|           null|               null|      null| null|\n",
      "|       61391|  76166|     UNIQUE|  CINGULAR WIRELESS|   TX|NOT ACCEPTABLE|32.72|-97.31| -0.1|-0.83| 0.54|         NA|     US|Cingular Wireless...|NA-US-TX-CINGULAR...|        FALSE|           null|               null|      null| null|\n",
      "|       61392|  76177|   STANDARD|         FORT WORTH|   TX|       PRIMARY|32.75|-97.33| -0.1|-0.83| 0.54|         NA|     US|      Fort Worth, TX| NA-US-TX-FORT WORTH|        FALSE|           2126|               4053| 122396986| null|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path=\"D:/PySpark/pyspark-examples-master/resources/\"\n",
    "\n",
    "df2=spark.read.option(\"header\",True).csv(path+\"zipcodes.csv\")\n",
    "\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caf5f5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----------+-------------------+-----+--------------+-----+------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-----+\n",
      "|RecordNumber|Zipcode|ZipCodeType|               City|State|  LocationType|  Lat|  Long|Xaxis|Yaxis|Zaxis|WorldRegion|Country|        LocationText|            Location|Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|Notes|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-----+\n",
      "|           1|    704|   STANDARD|        PARC PARQUE|   PR|NOT ACCEPTABLE|17.96|-66.22| 0.38|-0.87|  0.3|         NA|     US|     Parc Parque, PR|NA-US-PR-PARC PARQUE|        FALSE|           null|               null|      null| null|\n",
      "|           2|    704|   STANDARD|PASEO COSTA DEL SUR|   PR|NOT ACCEPTABLE|17.96|-66.22| 0.38|-0.87|  0.3|         NA|     US|Paseo Costa Del S...|NA-US-PR-PASEO CO...|        FALSE|           null|               null|      null| null|\n",
      "|          10|    709|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14|-66.26| 0.38|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        FALSE|           null|               null|      null| null|\n",
      "|       61391|  76166|     UNIQUE|  CINGULAR WIRELESS|   TX|NOT ACCEPTABLE|32.72|-97.31| -0.1|-0.83| 0.54|         NA|     US|Cingular Wireless...|NA-US-TX-CINGULAR...|        FALSE|           null|               null|      null| null|\n",
      "|       61392|  76177|   STANDARD|         FORT WORTH|   TX|       PRIMARY|32.75|-97.33| -0.1|-0.83| 0.54|         NA|     US|      Fort Worth, TX| NA-US-TX-FORT WORTH|        FALSE|           2126|               4053| 122396986| null|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=spark.read.option(\"header\",True).format(\"csv\").load(path+\"zipcodes.csv\")\n",
    "\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b4ab68",
   "metadata": {},
   "source": [
    "# Create an Empty DataFrame & RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2e5dc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empty rdd\n",
    "emptRDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "emptRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaaad1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emptyRDD = spark.sparkContext.parallelize([])\n",
    "\n",
    "emptyRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e43e7778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# empty dataframe\n",
    "schema = StructType([\n",
    "    StructField('firstname',StringType(),True),\n",
    "    StructField('middlename',StringType(),True),\n",
    "    StructField('lastname',StringType(),True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(emptyRDD,schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fff80981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+\n",
      "|firstname|middlename|lastname|\n",
      "+---------+----------+--------+\n",
      "+---------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "237c4ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+\n",
      "|firstname|middlename|lastname|\n",
      "+---------+----------+--------+\n",
      "+---------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert empty RDD to Dataframe\n",
    "df1 = emptyRDD.toDF(schema)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f5fad95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+\n",
      "|firstname|middlename|lastname|\n",
      "+---------+----------+--------+\n",
      "+---------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create empty DataFrame directly.\n",
    "df2 = spark.createDataFrame([],schema)\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac650450",
   "metadata": {},
   "source": [
    "# Convert PySpark DataFrame to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0197f339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- middle_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
    "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
    "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
    "\n",
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
    "\n",
    "pySparkDF = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "pySparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68ad1330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+-----+------+------+\n",
      "|first_name|middle_name|last_name|  dob|gender|salary|\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|     James|           |    Smith|36636|     M| 60000|\n",
      "|   Michael|       Rose|         |40288|     M| 70000|\n",
      "|    Robert|           | Williams|42114|      |400000|\n",
      "|     Maria|       Anne|    Jones|39192|     F|500000|\n",
      "|       Jen|       Mary|    Brown|     |     F|     0|\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pySparkDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4662843c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>middle_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>dob</th>\n",
       "      <th>gender</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>James</td>\n",
       "      <td></td>\n",
       "      <td>Smith</td>\n",
       "      <td>36636</td>\n",
       "      <td>M</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Michael</td>\n",
       "      <td>Rose</td>\n",
       "      <td></td>\n",
       "      <td>40288</td>\n",
       "      <td>M</td>\n",
       "      <td>70000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robert</td>\n",
       "      <td></td>\n",
       "      <td>Williams</td>\n",
       "      <td>42114</td>\n",
       "      <td></td>\n",
       "      <td>400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maria</td>\n",
       "      <td>Anne</td>\n",
       "      <td>Jones</td>\n",
       "      <td>39192</td>\n",
       "      <td>F</td>\n",
       "      <td>500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jen</td>\n",
       "      <td>Mary</td>\n",
       "      <td>Brown</td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_name middle_name last_name    dob gender  salary\n",
       "0      James                 Smith  36636      M   60000\n",
       "1    Michael        Rose            40288      M   70000\n",
       "2     Robert              Williams  42114         400000\n",
       "3      Maria        Anne     Jones  39192      F  500000\n",
       "4        Jen        Mary     Brown             F       0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandasDF = pySparkDF.toPandas()\n",
    "\n",
    "pandasDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d92c2372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------\n",
      " first_name  | James    \n",
      " middle_name |          \n",
      " last_name   | Smith    \n",
      " dob         | 36636    \n",
      " gender      | M        \n",
      " salary      | 60000    \n",
      "-RECORD 1---------------\n",
      " first_name  | Michael  \n",
      " middle_name | Rose     \n",
      " last_name   |          \n",
      " dob         | 40288    \n",
      " gender      | M        \n",
      " salary      | 70000    \n",
      "-RECORD 2---------------\n",
      " first_name  | Robert   \n",
      " middle_name |          \n",
      " last_name   | Williams \n",
      " dob         | 42114    \n",
      " gender      |          \n",
      " salary      | 400000   \n",
      "-RECORD 3---------------\n",
      " first_name  | Maria    \n",
      " middle_name | Anne     \n",
      " last_name   | Jones    \n",
      " dob         | 39192    \n",
      " gender      | F        \n",
      " salary      | 500000   \n",
      "-RECORD 4---------------\n",
      " first_name  | Jen      \n",
      " middle_name | Mary     \n",
      " last_name   | Brown    \n",
      " dob         |          \n",
      " gender      | F        \n",
      " salary      | 0        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pySparkDF.show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6019f37d",
   "metadata": {},
   "source": [
    "# Nested StructType object struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5456eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "structureData = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('id', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "df2=spark.createDataFrame(data=structureData,schema=structureSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ddd1dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+------+\n",
      "|                name|   id|gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|    [James, , Smith]|36636|     M|  3100|\n",
      "|   [Michael, Rose, ]|40288|     M|  4300|\n",
      "|[Robert, , Williams]|42114|     M|  1400|\n",
      "|[Maria, Anne, Jones]|39192|     F|  5500|\n",
      "|  [Jen, Mary, Brown]|     |     F|    -1|\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e6d2c",
   "metadata": {},
   "source": [
    "# Adding & Changing struct of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "735f3d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- OtherInfo: struct (nullable = false)\n",
      " |    |-- identifier: string (nullable = true)\n",
      " |    |-- Sex: string (nullable = true)\n",
      " |    |-- Salary: integer (nullable = true)\n",
      " |    |-- Salary_Grade: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,struct,when\n",
    "\n",
    "updatedDF= df2.withColumn(\"OtherInfo\",\n",
    "                         struct(col(\"id\").alias(\"identifier\"),\n",
    "                                col(\"gender\").alias(\"Sex\"),\n",
    "                                col(\"salary\").alias(\"Salary\"),\n",
    "                                when(col(\"salary\").cast(IntegerType())<2000,\"Low\")\\\n",
    "                               .when(col(\"salary\").cast(IntegerType())<4000,\"Medium\")\\\n",
    "                               .otherwise(\"High\").alias(\"Salary_Grade\")\n",
    "                               )).drop(\"id\",\"gender\",\"salary\")\n",
    "\n",
    "updatedDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "779e3383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+\n",
      "|name                |OtherInfo               |\n",
      "+--------------------+------------------------+\n",
      "|[James, , Smith]    |[36636, M, 3100, Medium]|\n",
      "|[Michael, Rose, ]   |[40288, M, 4300, High]  |\n",
      "|[Robert, , Williams]|[42114, M, 1400, Low]   |\n",
      "|[Maria, Anne, Jones]|[39192, F, 5500, High]  |\n",
      "|[Jen, Mary, Brown]  |[, F, -1, Low]          |\n",
      "+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updatedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1de30082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"fields\":[{\"metadata\":{},\"name\":\"name\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"firstname\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"middlename\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"lastname\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"id\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"gender\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"salary\",\"nullable\":true,\"type\":\"integer\"}],\"type\":\"struct\"}\n"
     ]
    }
   ],
   "source": [
    "print(df2.schema.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10e44054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "struct<name:struct<firstname:string,middlename:string,lastname:string>,id:string,gender:string,salary:int>\n"
     ]
    }
   ],
   "source": [
    "print(df2.schema.simpleString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c4d46",
   "metadata": {},
   "source": [
    "# Row Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d0c7f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kazmi,40\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row=Row(\"Kazmi\",40)\n",
    "\n",
    "print(row[0]+\",\"+str(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25f59532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamza\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# with names arguments\n",
    "row=Row(name=\"Hamza\",age=10)\n",
    "\n",
    "print(row.name)\n",
    "print(row.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e9134",
   "metadata": {},
   "source": [
    "# Custom Class from Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7007f8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James\n",
      "Alice\n"
     ]
    }
   ],
   "source": [
    "Person = Row(\"name\",\"age\")\n",
    "p1=Person(\"James\",40)\n",
    "p2=Person(\"Alice\",35)\n",
    "\n",
    "print(p1.name)\n",
    "print(p2.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d026b6ff",
   "metadata": {},
   "source": [
    "# Row class on PySpark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70245769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lang=['Java', 'Scala', 'C++'], name='James,,Smith', state='CA'),\n",
       " Row(lang=['Spark', 'Java', 'C++'], name='Michael,Rose,', state='NJ'),\n",
       " Row(lang=['CSharp', 'VB'], name='Robert,,Williams', state='NV')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [Row(name=\"James,,Smith\",lang=[\"Java\",\"Scala\",\"C++\"],state=\"CA\"), \n",
    "        Row(name=\"Michael,Rose,\",lang=[\"Spark\",\"Java\",\"C++\"],state=\"NJ\"),\n",
    "        Row(name=\"Robert,,Williams\",lang=[\"CSharp\",\"VB\"],state=\"NV\")]\n",
    "\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "\n",
    "rdd.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d63393e",
   "metadata": {},
   "source": [
    "# Row class on PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0cebc26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9cfddf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lang: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "681d4f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+-----+\n",
      "|              lang|            name|state|\n",
      "+------------------+----------------+-----+\n",
      "|[Java, Scala, C++]|    James,,Smith|   CA|\n",
      "|[Spark, Java, C++]|   Michael,Rose,|   NJ|\n",
      "|      [CSharp, VB]|Robert,,Williams|   NV|\n",
      "+------------------+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8ca61de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- languagesAtSchool: string (nullable = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# changing column names\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "df=spark.createDataFrame(data).toDF(*columns)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b83465",
   "metadata": {},
   "source": [
    "# Nested Struct Using Row Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4feb941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "| name|          prop|\n",
      "+-----+--------------+\n",
      "|james|[black, black]|\n",
      "|  ann|  [blue, grey]|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[Row(name=\"james\",prop=Row(hair=\"black\",eye=\"black\")),\n",
    "      Row(name=\"ann\",prop=Row(hair=\"grey\",eye=\"blue\"))\n",
    "     ]\n",
    "\n",
    "df=spark.createDataFrame(data)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11290623",
   "metadata": {},
   "source": [
    "# Column Class Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d8cc173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.column.Column'>\n",
      "Column<b'col'>\n"
     ]
    }
   ],
   "source": [
    "# create a Column class object is by using lit\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "colObj = lit(\"col\")\n",
    "\n",
    "print(type(colObj))\n",
    "print(colObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d41dc0",
   "metadata": {},
   "source": [
    "# Access the Column from DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44cef4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[(\"James\",23),(\"Ann\",40)]\n",
    "\n",
    "df=spark.createDataFrame(data).toDF(\"name\",\"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6013dc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|    23|\n",
      "|    40|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.gender).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5cc970e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|    23|\n",
      "|    40|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"gender\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3db9d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|    23|\n",
      "|    40|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"gender\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad17cc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|    23|\n",
      "|    40|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"gender\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d809bda",
   "metadata": {},
   "source": [
    "# Column Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b462b895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "| 100|   2|   1|\n",
      "| 200|   3|   4|\n",
      "| 300|   4|   4|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(100,2,1),(200,3,4),(300,4,4)]\n",
    "\n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c62020a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(col1 + col2)|\n",
      "+-------------+\n",
      "|          102|\n",
      "|          203|\n",
      "|          304|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.col1 + df.col2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d196fef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(col1 - col2)|\n",
      "+-------------+\n",
      "|           98|\n",
      "|          197|\n",
      "|          296|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.col1 - df.col2).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "28c83d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(col1 * col2)|\n",
      "+-------------+\n",
      "|          200|\n",
      "|          600|\n",
      "|         1200|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.col1 * df.col2).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a1bd9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    (col1 / col2)|\n",
      "+-----------------+\n",
      "|             50.0|\n",
      "|66.66666666666667|\n",
      "|             75.0|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.col1 / df.col2).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b3c759bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(col1 % col2)|\n",
      "+-------------+\n",
      "|            0|\n",
      "|            2|\n",
      "|            0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.col1 % df.col2).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29f2247e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(col2 > col3)|\n",
      "+-------------+\n",
      "|         true|\n",
      "|        false|\n",
      "|        false|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.col2 > df.col3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0371ef0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(col2 < col3)|\n",
      "+-------------+\n",
      "|        false|\n",
      "|         true|\n",
      "|        false|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.col2 < df.col3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bda02a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(col2 = col3)|\n",
      "+-------------+\n",
      "|        false|\n",
      "|        false|\n",
      "|         true|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.col2 == df.col3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6daec69",
   "metadata": {},
   "source": [
    "# Column Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e3047bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[(\"James\",\"Bond\",\"100\",None),\n",
    "      (\"Ann\",\"Varsa\",\"200\",'F'),\n",
    "      (\"Tom Cruise\",\"XXX\",\"400\",''),\n",
    "      (\"Tom Brand\",None,\"400\",'M')] \n",
    "\n",
    "columns=[\"fname\",\"lname\",\"id\",\"gender\"]\n",
    "\n",
    "df=spark.createDataFrame(data,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a38dddc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     James|     Bond|\n",
      "|       Ann|    Varsa|\n",
      "|Tom Cruise|      XXX|\n",
      "| Tom Brand|     null|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df.select(df[\"fname\"].alias(\"first_name\"),\n",
    "          df[\"lname\"].alias(\"last_name\")\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e3ab9907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|      fullname|\n",
      "+--------------+\n",
      "|    James,Bond|\n",
      "|     Ann,Varsa|\n",
      "|Tom Cruise,XXX|\n",
      "|          null|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"fname ||','||lname\").alias(\"fullname\")\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f983f45",
   "metadata": {},
   "source": [
    "# Ascending or Descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0532e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|       Ann|Varsa|200|     F|\n",
      "|     James| Bond|100|  null|\n",
      "| Tom Brand| null|400|     M|\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "+----------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df[\"fname\"].asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "54932480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "| Tom Brand| null|400|     M|\n",
      "|     James| Bond|100|  null|\n",
      "|       Ann|Varsa|200|     F|\n",
      "+----------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df[\"fname\"].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c0b3c9",
   "metadata": {},
   "source": [
    "# cast() & astype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "722abfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"fname\"],df[\"id\"].cast(\"int\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2736bf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"fname\"],df[\"id\"].astype(\"int\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d0a3c",
   "metadata": {},
   "source": [
    "# between()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0014b6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+------+\n",
      "|fname|lname| id|gender|\n",
      "+-----+-----+---+------+\n",
      "|James| Bond|100|  null|\n",
      "|  Ann|Varsa|200|     F|\n",
      "+-----+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"id\"].between(100,300)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3fdb3e",
   "metadata": {},
   "source": [
    "# contains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c806a55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "+----------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"fname\"].contains(\"Cruise\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071ade4",
   "metadata": {},
   "source": [
    "# startswith() & endswith()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f884fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "| Tom Brand| null|400|     M|\n",
      "+----------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"fname\"].startswith(\"T\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1de8579f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "+----------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"fname\"].endswith(\"Cruise\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd04a3ca",
   "metadata": {},
   "source": [
    "# isNull & isNotNull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b26f6238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---+------+\n",
      "|    fname|lname| id|gender|\n",
      "+---------+-----+---+------+\n",
      "|Tom Brand| null|400|     M|\n",
      "+---------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"lname\"].isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1276d11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|     James| Bond|100|  null|\n",
      "|       Ann|Varsa|200|     F|\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "+----------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"lname\"].isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a5eac2",
   "metadata": {},
   "source": [
    "# like()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9e38c925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "|fname|lname| id|\n",
      "+-----+-----+---+\n",
      "|  Ann|Varsa|200|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"fname\"],df[\"lname\"],df[\"id\"]).filter(df[\"fname\"].like(\"%n\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e8ddf",
   "metadata": {},
   "source": [
    "# substr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4c39b49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|subtr|\n",
      "+-----+\n",
      "|   Ja|\n",
      "|   An|\n",
      "|   To|\n",
      "|   To|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"fname\"].substr(1,2).alias(\"subtr\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaa6ef7",
   "metadata": {},
   "source": [
    "# when() & otherwise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0a1e8b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+\n",
      "|     fname|lname|new_gender|\n",
      "+----------+-----+----------+\n",
      "|     James| Bond|      null|\n",
      "|       Ann|Varsa|    Female|\n",
      "|Tom Cruise|  XXX|          |\n",
      "| Tom Brand| null|      Male|\n",
      "+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df.select(df[\"fname\"],df[\"lname\"],when(df[\"gender\"]==\"M\",\"Male\")\n",
    "                                 .when(df[\"gender\"]==\"F\",\"Female\")\n",
    "                                 .when(df[\"gender\"]==None,\"\")\n",
    "                                 .otherwise(df[\"gender\"]).alias(\"new_gender\")\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919789e",
   "metadata": {},
   "source": [
    "# isin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6e634d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "|fname|lname| id|\n",
      "+-----+-----+---+\n",
      "|James| Bond|100|\n",
      "|  Ann|Varsa|200|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "li=[\"100\",\"200\"]\n",
    "df.select(df[\"fname\"],df[\"lname\"],df[\"id\"]).filter(df[\"id\"].isin(li)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1763c7a1",
   "metadata": {},
   "source": [
    "# getField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e00c4571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+--------------------+\n",
      "|          name|      languages|          properties|\n",
      "+--------------+---------------+--------------------+\n",
      "| [James, Bond]|     [Java, C#]|[eye -> brown, ha...|\n",
      "|  [Ann, Varsa]| [.NET, Python]|[eye -> black, ha...|\n",
      "|[Tom Cruise, ]|[Python, Scala]|[eye -> grey, hai...|\n",
      "|  [Tom Brand,]|   [Perl, Ruby]|[eye -> blue, hai...|\n",
      "+--------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,ArrayType,MapType\n",
    "data=[((\"James\",\"Bond\"),[\"Java\",\"C#\"],{'hair':'black','eye':'brown'}),\n",
    "      ((\"Ann\",\"Varsa\"),[\".NET\",\"Python\"],{'hair':'brown','eye':'black'}),\n",
    "      ((\"Tom Cruise\",\"\"),[\"Python\",\"Scala\"],{'hair':'red','eye':'grey'}),\n",
    "      ((\"Tom Brand\",None),[\"Perl\",\"Ruby\"],{'hair':'black','eye':'blue'})]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField('name', StructType([\n",
    "            StructField('fname', StringType(), True),\n",
    "            StructField('lname', StringType(), True)])),\n",
    "        StructField('languages', ArrayType(StringType()),True),\n",
    "        StructField('properties', MapType(StringType(),StringType()),True)\n",
    "     ])\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "30e5f33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|properties[hair]|\n",
      "+----------------+\n",
      "|           black|\n",
      "|           brown|\n",
      "|             red|\n",
      "|           black|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"properties\"].getField(\"hair\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b8b9211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|name.fname|\n",
      "+----------+\n",
      "|     James|\n",
      "|       Ann|\n",
      "|Tom Cruise|\n",
      "| Tom Brand|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"name\"].getField(\"fname\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8589672",
   "metadata": {},
   "source": [
    "# getItem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7208d263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|languages[1]|\n",
      "+------------+\n",
      "|          C#|\n",
      "|      Python|\n",
      "|       Scala|\n",
      "|        Ruby|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"languages\"].getItem(1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bbb8246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|properties[hair]|\n",
      "+----------------+\n",
      "|           black|\n",
      "|           brown|\n",
      "|             red|\n",
      "|           black|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"properties\"].getItem(\"hair\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdd617",
   "metadata": {},
   "source": [
    "# select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cfb785fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|     fname|lname|\n",
      "+----------+-----+\n",
      "|     James| Bond|\n",
      "|       Ann|Varsa|\n",
      "|Tom Cruise|     |\n",
      "| Tom Brand| null|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name.fname\",\"name.lname\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f9b96cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|     fname|lname|\n",
      "+----------+-----+\n",
      "|     James| Bond|\n",
      "|       Ann|Varsa|\n",
      "|Tom Cruise|     |\n",
      "| Tom Brand| null|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name.*\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a79eba4",
   "metadata": {},
   "source": [
    "# collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3b12b63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "\n",
    "deptDF=spark.createDataFrame(data=data,schema=deptColumns)\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2dceb51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n"
     ]
    }
   ],
   "source": [
    "dataCollect=deptDF.collect()\n",
    "print(dataCollect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b8667277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finance,10\n",
      "Marketing,20\n",
      "Sales,30\n",
      "IT,40\n"
     ]
    }
   ],
   "source": [
    "for row in dataCollect:\n",
    "    print(row['dept_name']+\",\"+str(row[\"dept_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "638e2257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finance'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deptDF.collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41c3100",
   "metadata": {},
   "source": [
    "# withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ce606634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "\n",
    "df=spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5c6e3fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f8a31897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"salary\",col(\"salary\").cast(\"string\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0eb78fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "80dc902c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+-------+\n",
      "|firstname|middlename|lastname|       dob|gender| salary|\n",
      "+---------+----------+--------+----------+------+-------+\n",
      "|    James|          |   Smith|1991-04-01|     M|3000000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|4000000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|4000000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|4000000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|  -1000|\n",
      "+---------+----------+--------+----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# updated value of existing column\n",
    "df.withColumn(\"salary\",col(\"salary\")*1000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7950e7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+------------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|CopiedColumn|\n",
      "+---------+----------+--------+----------+------+------+------------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|       -3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|       -4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|       -4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|       -4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|           1|\n",
      "+---------+----------+--------+----------+------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a new column \n",
    "df.withColumn(\"CopiedColumn\",col(\"salary\")*-1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c027bfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+-------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|Country|\n",
      "+---------+----------+--------+----------+------+------+-------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|    USA|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|    USA|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|    USA|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|    USA|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|    USA|\n",
      "+---------+----------+--------+----------+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Country\",lit(\"USA\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf6b503",
   "metadata": {},
   "source": [
    "# withColumnRenamed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "38ac6e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+---+------+\n",
      "|firstname|middlename|lastname|dob       |sex|salary|\n",
      "+---------+----------+--------+----------+---+------+\n",
      "|James    |          |Smith   |1991-04-01|M  |3000  |\n",
      "|Michael  |Rose      |        |2000-05-19|M  |4000  |\n",
      "|Robert   |          |Williams|1978-09-05|M  |4000  |\n",
      "|Maria    |Anne      |Jones   |1967-12-01|F  |4000  |\n",
      "|Jen      |Mary      |Brown   |1980-02-17|F  |-1    |\n",
      "+---------+----------+--------+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"gender\",\"sex\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c79d44a",
   "metadata": {},
   "source": [
    "# drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fb4cd891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|\n",
      "+---------+----------+--------+----------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|\n",
      "|   Robert|          |Williams|1978-09-05|     M|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|\n",
      "+---------+----------+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad9d787",
   "metadata": {},
   "source": [
    "# distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0be6ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "data = [(\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600), \\\n",
    "    (\"Robert\", \"Sales\", 4100), \\\n",
    "    (\"Maria\", \"Finance\", 3000), \\\n",
    "    (\"James\", \"Sales\", 3000), \\\n",
    "    (\"Scott\", \"Finance\", 3300), \\\n",
    "    (\"Jen\", \"Finance\", 3900), \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000), \\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  ]\n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "\n",
    "df=spark.createDataFrame(data=data,schema=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5db5c96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9ea4b4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4da0194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count: 9\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|Jen          |Finance   |3900  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distinctDF=df.distinct()\n",
    "print(\"Distinct count: \"+str(distinctDF.count()))\n",
    "distinctDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ba1c8",
   "metadata": {},
   "source": [
    "# dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c1e71031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count: 9\n",
      "Duplicate count: 10\n"
     ]
    }
   ],
   "source": [
    "df2=df.dropDuplicates()\n",
    "print(\"Distinct count: \"+str(df2.count()))\n",
    "print(\"Duplicate count: \"+str(df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bb2b9c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discount count of department & salary: 8\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropDisDF=df.dropDuplicates([\"department\",\"salary\"])\n",
    "print(\"Discount count of department & salary: \"+str(dropDisDF.count()))\n",
    "dropDisDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e995b",
   "metadata": {},
   "source": [
    "# orderBy() and sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "63e9cc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df=spark.createDataFrame(data=simpleData,schema=columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1cb1e616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"department\",\"state\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7217a31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(col(\"department\"),col(\"state\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "698f1245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"department\",\"state\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "47c5f1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df[\"department\"].asc(),df[\"state\"].asc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e081c427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df[\"department\"].desc(),df[\"state\"].desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9329d920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\"select employee_name,department,state,salary,age,bonus from EMP ORDER BY department asc\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc03266a",
   "metadata": {},
   "source": [
    "# groupBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7fa89b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "count() - Returns the count of rows for each group.\n",
    "\n",
    "mean() - Returns the mean of values for each group.\n",
    "\n",
    "max() - Returns the maximum of values for each group.\n",
    "\n",
    "min() - Returns the minimum of values for each group.\n",
    "\n",
    "sum() - Returns the total for values for each group.\n",
    "\n",
    "avg() - Returns the average for values for each group.\n",
    "\n",
    "agg() - Using agg() function, we can calculate more than one aggregate at a time.\n",
    "\n",
    "pivot()\n",
    "\"\"\"\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "11605f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|Sales     |257000     |\n",
      "|Finance   |351000     |\n",
      "|Marketing |171000     |\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "afb954c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------+\n",
      "|department|state|sum(salary)|sum(bonus)|\n",
      "+----------+-----+-----------+----------+\n",
      "|   Finance|   NY|     162000|     34000|\n",
      "| Marketing|   NY|      91000|     21000|\n",
      "|     Sales|   CA|      81000|     23000|\n",
      "| Marketing|   CA|      80000|     18000|\n",
      "|   Finance|   CA|     189000|     47000|\n",
      "|     Sales|   NY|     176000|     30000|\n",
      "+----------+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\",\"state\").sum(\"salary\",\"bonus\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cf92e0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+---------+---------+\n",
      "|department|sum_salry|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+---------+-----------------+---------+---------+\n",
      "|Sales     |257000   |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000   |87750.0          |81000    |24000    |\n",
      "|Marketing |171000   |85500.0          |39000    |21000    |\n",
      "+----------+---------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running more aggregates at a time\n",
    "from pyspark.sql.functions import sum,avg,max,min,mean,count\n",
    "\n",
    "df.groupBy(\"department\").agg(sum(\"salary\").alias(\"sum_salry\")\\\n",
    "                            ,avg(\"salary\").alias(\"avg_salary\")\\\n",
    "                            ,sum(\"bonus\").alias(\"sum_bonus\")\\\n",
    "                            ,max(\"bonus\").alias(\"max_bonus\")\n",
    "                            ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b9a04",
   "metadata": {},
   "source": [
    "# Using filter on aggregate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0d908220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+---------+---------+\n",
      "|department|sum_salry|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+---------+-----------------+---------+---------+\n",
      "|Sales     |257000   |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000   |87750.0          |81000    |24000    |\n",
      "+----------+---------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").agg(sum(\"salary\").alias(\"sum_salry\")\\\n",
    "                            ,avg(\"salary\").alias(\"avg_salary\")\\\n",
    "                            ,sum(\"bonus\").alias(\"sum_bonus\")\\\n",
    "                            ,max(\"bonus\").alias(\"max_bonus\")\n",
    "                            ).where(col(\"sum_bonus\")>=50000).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27befa3",
   "metadata": {},
   "source": [
    "# JOINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f19c797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "\n",
    "empDF=spark.createDataFrame(data=emp,schema=empColumns)\n",
    "deptDF=spark.createDataFrame(data=dept,schema=deptColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "271aaee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "51a9265d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae6425f",
   "metadata": {},
   "source": [
    "# INNER JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6eeb5686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id==deptDF.dept_id,\"inner\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbaa1e6",
   "metadata": {},
   "source": [
    "# OUTER JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1cbde27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id==deptDF.dept_id,\"outer\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b5e7a11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id==deptDF.dept_id,\"full\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f91de236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id==deptDF.dept_id,\"fullouter\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623b3f2e",
   "metadata": {},
   "source": [
    "# Left Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "29444780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id==deptDF.dept_id,\"left\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "39e4cce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id==deptDF.dept_id,\"right\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87c181e",
   "metadata": {},
   "source": [
    "# Leftsemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d1753d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "leftsemi join is similar to inner join difference being leftsemi join returns all columns from the left dataset and ignores\n",
    "all columns from the right dataset. In other words, this join returns columns from the only left dataset for the records match\n",
    "in the right dataset on join expression, records not matched on join expression are ignored from both left and right datasets.\n",
    "\"\"\"\n",
    "empDF.join(deptDF,empDF.emp_dept_id==deptDF.dept_id,\"leftsemi\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db5db95",
   "metadata": {},
   "source": [
    "# Left Anti Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "796582b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|6     |Brown|2              |2010       |50         |      |-1    |\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "leftanti join does the exact opposite of the leftsemi, leftanti join returns only columns from the left dataset \n",
    "for non-matched records.\n",
    "\"\"\"\n",
    "empDF.join(deptDF,empDF.emp_dept_id==deptDF.dept_id,\"leftanti\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f49dd",
   "metadata": {},
   "source": [
    "# Self Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2f81b525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------------+\n",
      "|emp_id|name    |superior_emp_id|superior_emp_name|\n",
      "+------+--------+---------------+-----------------+\n",
      "|2     |Rose    |1              |Smith            |\n",
      "|3     |Williams|1              |Smith            |\n",
      "|4     |Jones   |2              |Rose             |\n",
      "|5     |Brown   |2              |Rose             |\n",
      "|6     |Brown   |2              |Rose             |\n",
      "+------+--------+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\")\\\n",
    "                        ,col(\"emp1.superior_emp_id\")==col(\"emp2.emp_id\")\n",
    "                        ,\"inner\").select(\n",
    "                                    col(\"emp1.emp_id\"),col(\"emp1.name\")\\\n",
    "                                    ,col(\"emp2.emp_id\").alias(\"superior_emp_id\")\\\n",
    "                                    ,col(\"emp2.name\").alias(\"superior_emp_name\")\n",
    "                                ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e561813f",
   "metadata": {},
   "source": [
    "# Union and UnionAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fbf00566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "655f5e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e827b8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unionDF=df.union(df2)\n",
    "unionDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "098733ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame unionAll() method is deprecated since PySpark 2.0.0 version and recommends using the union() method\n",
    "unionAllDF=df.unionAll(df2)\n",
    "unionAllDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "04417079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this how we can do only union\n",
    "disDF=df.union(df2).distinct()\n",
    "disDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0576c5",
   "metadata": {},
   "source": [
    "# Merge Two DataFrames with Different Columns or Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2c26b766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"Sales\",34), (\"Michael\",\"Sales\",56), \\\n",
    "    (\"Robert\",\"Sales\",30), (\"Maria\",\"Finance\",24) ]\n",
    "columns= [\"name\",\"dept\",\"age\"]\n",
    "\n",
    "df1=spark.createDataFrame(data=data,schema=columns)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c188794c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---+\n",
      "|   name|   dept|age|\n",
      "+-------+-------+---+\n",
      "|  James|  Sales| 34|\n",
      "|Michael|  Sales| 56|\n",
      "| Robert|  Sales| 30|\n",
      "|  Maria|Finance| 24|\n",
      "+-------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "27e69b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2=[\n",
    "       (\"James\",\"Sales\",\"NY\",9000),\n",
    "       (\"Maria\",\"Finance\",\"CA\",9000), \\\n",
    "       (\"Jen\",\"Finance\",\"NY\",7900),\n",
    "       (\"Jeff\",\"Marketing\",\"CA\",8000)\n",
    "      ]\n",
    "columns2= [\"name\",\"dept\",\"state\",\"salary\"]\n",
    "\n",
    "df2=spark.createDataFrame(data=data2,schema=columns2)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c892a4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----+------+\n",
      "| name|     dept|state|salary|\n",
      "+-----+---------+-----+------+\n",
      "|James|    Sales|   NY|  9000|\n",
      "|Maria|  Finance|   CA|  9000|\n",
      "|  Jen|  Finance|   NY|  7900|\n",
      "| Jeff|Marketing|   CA|  8000|\n",
      "+-----+---------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7293028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [column for column in df2.columns if column not in df1.columns]:\n",
    "    df1 = df1.withColumn(column, lit(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "62ec4d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---+-----+------+\n",
      "|   name|   dept|age|state|salary|\n",
      "+-------+-------+---+-----+------+\n",
      "|  James|  Sales| 34| null|  null|\n",
      "|Michael|  Sales| 56| null|  null|\n",
      "| Robert|  Sales| 30| null|  null|\n",
      "|  Maria|Finance| 24| null|  null|\n",
      "+-------+-------+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f034aa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----+------+----+\n",
      "| name|     dept|state|salary| age|\n",
      "+-----+---------+-----+------+----+\n",
      "|James|    Sales|   NY|  9000|null|\n",
      "|Maria|  Finance|   CA|  9000|null|\n",
      "|  Jen|  Finance|   NY|  7900|null|\n",
      "| Jeff|Marketing|   CA|  8000|null|\n",
      "+-----+---------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in [column for column in df1.columns if column not in df2.columns]:\n",
    "    df2 = df2.withColumn(column, lit(None))\n",
    "    \n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "448f3c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----+-----+------+\n",
      "|   name|     dept| age|state|salary|\n",
      "+-------+---------+----+-----+------+\n",
      "|  James|    Sales|  34| null|  null|\n",
      "|Michael|    Sales|  56| null|  null|\n",
      "| Robert|    Sales|  30| null|  null|\n",
      "|  Maria|  Finance|  24| null|  null|\n",
      "|  James|    Sales|null|   NY|  9000|\n",
      "|  Maria|  Finance|null|   CA|  9000|\n",
      "|    Jen|  Finance|null|   NY|  7900|\n",
      "|   Jeff|Marketing|null|   CA|  8000|\n",
      "+-------+---------+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df=df1.unionByName(df2)\n",
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07feb72",
   "metadata": {},
   "source": [
    "# map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "366a00a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project',\n",
       " 'Gutenbergs',\n",
       " 'Alices',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'Project',\n",
       " 'Gutenbergs',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'Project',\n",
       " 'Gutenbergs']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"Project\",\"Gutenbergs\",\"Alices\",\"Adventures\",\n",
    "\"in\",\"Wonderland\",\"Project\",\"Gutenbergs\",\"Adventures\",\n",
    "\"in\",\"Wonderland\",\"Project\",\"Gutenbergs\"]\n",
    "\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1bc01e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project', 1)\n",
      "('Gutenbergs', 1)\n",
      "('Alices', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenbergs', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenbergs', 1)\n"
     ]
    }
   ],
   "source": [
    "rdd2=rdd.map(lambda x:(x,1))\n",
    "\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f1509c",
   "metadata": {},
   "source": [
    "# map() with DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a33ec1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n",
      "|firstname|lastname|gender|salary|\n",
      "+---------+--------+------+------+\n",
      "|    James|   Smith|     M|    30|\n",
      "|     Anna|    Rose|     F|    41|\n",
      "|   Robert|Williams|     M|    62|\n",
      "+---------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James','Smith','M',30),\n",
    "  ('Anna','Rose','F',41),\n",
    "  ('Robert','Williams','M',62), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "\n",
    "df=spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "166fb0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------+\n",
      "|           name|gender|new_salary|\n",
      "+---------------+------+----------+\n",
      "|    James,Smith|     M|        60|\n",
      "|      Anna,Rose|     F|        82|\n",
      "|Robert,Williams|     M|       124|\n",
      "+---------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd2=df.rdd.map(lambda x:(x[0]+\",\"+x[1],x[2],x[3]*2))\n",
    "\n",
    "df2=rdd2.toDF([\"name\",\"gender\",\"new_salary\"])\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8fd2bc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James,Smith', 'm', 60),\n",
       " ('Anna,Rose', 'f', 82),\n",
       " ('Robert,Williams', 'm', 124)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func1(x):\n",
    "    firstName=x.firstname\n",
    "    lastName=x.lastname\n",
    "    name=firstName+\",\"+lastName\n",
    "    gender=x.gender.lower()\n",
    "    salary=x.salary*2\n",
    "    return (name,gender,salary)\n",
    "\n",
    "rdd2=df.rdd.map(lambda x: func1(x))\n",
    "\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446e9f6",
   "metadata": {},
   "source": [
    "# flatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "10a5c15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project Gutenbergs',\n",
       " 'Alices Adventures in Wonderland',\n",
       " 'Project Gutenbergs',\n",
       " 'Adventures in Wonderland',\n",
       " 'Project Gutenbergs']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"Project Gutenbergs\",\n",
    "        \"Alices Adventures in Wonderland\",\n",
    "        \"Project Gutenbergs\",\n",
    "        \"Adventures in Wonderland\",\n",
    "        \"Project Gutenbergs\"]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4627d591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project',\n",
       " 'Gutenbergs',\n",
       " 'Alices',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'Project',\n",
       " 'Gutenbergs',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'Project',\n",
       " 'Gutenbergs']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2=rdd.flatMap(lambda x:x.split(\" \"))\n",
    "\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "eca4baa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Project', 'Gutenbergs'],\n",
       " ['Alices', 'Adventures', 'in', 'Wonderland'],\n",
       " ['Project', 'Gutenbergs'],\n",
       " ['Adventures', 'in', 'Wonderland'],\n",
       " ['Project', 'Gutenbergs']]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3=rdd.map(lambda x:x.split(\" \"))\n",
    "\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df7c785",
   "metadata": {},
   "source": [
    "# fillna() & fill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ba891b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "|  2|    704|    null|PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               null|   TX|      null|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path=\"D:/PySpark/pyspark-examples-master/resources/\"\n",
    "\n",
    "df=spark.read.format(\"csv\").options(header=\"true\",inferSchema=\"true\").load(path+\"small_zipcode.csv\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a101f021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "|  2|    704|    null|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               null|   TX|         0|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(value=0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1c363a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|            unknown|   PR|     30100|\n",
      "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|            unknown|   TX|      null|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(\"unknown\",[\"city\"])\\\n",
    ".na.fill(\"\",[\"type\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8fdce1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|            unknown|   PR|     30100|\n",
      "|  2|    704|      NA|PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|    709|      NA|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|            unknown|   TX|      null|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill({\"city\":\"unknown\",\"type\":\"NA\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aee3a0",
   "metadata": {},
   "source": [
    "# pivot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3d8ecea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Amount: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
    "\n",
    "columns= [\"Product\",\"Amount\",\"Country\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "943cb40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "| Banana|  1000|    USA|\n",
      "|Carrots|  1500|    USA|\n",
      "|  Beans|  1600|    USA|\n",
      "| Orange|  2000|    USA|\n",
      "| Orange|  2000|    USA|\n",
      "| Banana|   400|  China|\n",
      "|Carrots|  1200|  China|\n",
      "|  Beans|  1500|  China|\n",
      "| Orange|  4000|  China|\n",
      "| Banana|  2000| Canada|\n",
      "|Carrots|  2000| Canada|\n",
      "|  Beans|  2000| Mexico|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ba0e0667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|Product|sum(Amount)|\n",
      "+-------+-----------+\n",
      "| Orange|       8000|\n",
      "|  Beans|       5100|\n",
      "| Banana|       3400|\n",
      "|Carrots|       4700|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Product\").sum(\"Amount\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a9d040c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Canada: long (nullable = true)\n",
      " |-- China: long (nullable = true)\n",
      " |-- Mexico: long (nullable = true)\n",
      " |-- USA: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivotDF=df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
    "pivotDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "512c4313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico|USA |\n",
      "+-------+------+-----+------+----+\n",
      "|Orange |null  |4000 |null  |4000|\n",
      "|Beans  |null  |1500 |2000  |1600|\n",
      "|Banana |2000  |400  |null  |1000|\n",
      "|Carrots|2000  |1200 |null  |1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivotDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e5eda852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+------+------+\n",
      "|Product|USA |China|Canada|Mexico|\n",
      "+-------+----+-----+------+------+\n",
      "|Orange |4000|4000 |null  |null  |\n",
      "|Beans  |1600|1500 |null  |2000  |\n",
      "|Banana |1000|400  |2000  |null  |\n",
      "|Carrots|1500|1200 |2000  |null  |\n",
      "+-------+----+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries = [\"USA\",\"China\",\"Canada\",\"Mexico\"]\n",
    "pivotDF = df.groupBy(\"Product\").pivot(\"Country\", countries).sum(\"Amount\")\n",
    "pivotDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "899d89ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico|USA |\n",
      "+-------+------+-----+------+----+\n",
      "|Orange |null  |4000 |null  |4000|\n",
      "|Beans  |null  |1500 |2000  |1600|\n",
      "|Banana |2000  |400  |null  |1000|\n",
      "|Carrots|2000  |1200 |null  |1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivotDF = df.groupBy(\"Product\",\"Country\") \\\n",
    "      .sum(\"Amount\") \\\n",
    "      .groupBy(\"Product\") \\\n",
    "      .pivot(\"Country\") \\\n",
    "      .sum(\"sum(Amount)\") \n",
    "pivotDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cedacb",
   "metadata": {},
   "source": [
    "# partitionBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4d99d71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.format(\"csv\").option(\"header\",True).load(path+\"simple-zipcodes.csv\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f354f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option(\"header\",True).partitionBy(\"state\").mode(\"overwrite\").csv(path+\"/output_data/zipcodes-state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5d6eb19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by multiple columns\n",
    "df.write.option(\"header\",True).partitionBy(\"state\",\"city\").mode(\"overwrite\").csv(path+\"/output_data/zipcodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6ec4883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option(\"header\",True).option(\"maxRecordsPerFile\",2)\\\n",
    ".partitionBy(\"state\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".csv(path+\"/output_data/zipcodes_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "00562dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------------+-------+-----+\n",
      "|RecordNumber|Country|               City|Zipcode|state|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|       49347|     US|               HOLT|  32564|   FL|\n",
      "|       49348|     US|          HOMOSASSA|  34487|   FL|\n",
      "|       49345|     US|           HILLIARD|  32046|   FL|\n",
      "|       49346|     US|             HOLDER|  34445|   FL|\n",
      "|       61391|     US|  CINGULAR WIRELESS|  76166|   TX|\n",
      "|       61392|     US|         FORT WORTH|  76177|   TX|\n",
      "|       61393|     US|           FT WORTH|  76177|   TX|\n",
      "|       54356|     US|        SPRUCE PINE|  35585|   AL|\n",
      "|       54354|     US|      SPRING GARDEN|  36275|   AL|\n",
      "|       54355|     US|        SPRINGVILLE|  35146|   AL|\n",
      "|       39827|     US|               MESA|  85209|   AZ|\n",
      "|       39828|     US|               MESA|  85210|   AZ|\n",
      "|       76511|     US|           ASH HILL|  27007|   NC|\n",
      "|       76512|     US|           ASHEBORO|  27203|   NC|\n",
      "|       76513|     US|           ASHEBORO|  27204|   NC|\n",
      "|           1|     US|        PARC PARQUE|    704|   PR|\n",
      "|           2|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
      "|          10|     US|       BDA SAN LUIS|    709|   PR|\n",
      "|           4|     US|    URB EUGENE RICE|    704|   PR|\n",
      "|           3|     US|      SECT LANAUSSE|    704|   PR|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read partition data\n",
    "parDF=spark.read.format(\"csv\").option(\"header\",True).load(path+\"output_data/zipcodes-state\")\n",
    "\n",
    "parDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ab08fa",
   "metadata": {},
   "source": [
    "# ArraType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "706f73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType,ArrayType\n",
    "\n",
    "arrayCol=ArrayType(StringType(),False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "19e057ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.ArrayType"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(arrayCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "60c88838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- languagesAtWork: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      " |-- previousState: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "                        StructField(\"name\",StringType(),True),\n",
    "                        StructField(\"languagesAtSchool\",ArrayType(StringType()),True),\n",
    "                        StructField(\"languagesAtWork\",ArrayType(StringType()),True),\n",
    "                        StructField(\"currentState\",StringType(),True),\n",
    "                        StructField(\"previousState\",StringType(),True)  ])\n",
    "\n",
    "df=spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "82220a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+---------------+------------+-------------+\n",
      "|            name| languagesAtSchool|languagesAtWork|currentState|previousState|\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8d3728",
   "metadata": {},
   "source": [
    "# explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "663c2489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|            name|   col|\n",
      "+----------------+------+\n",
      "|    James,,Smith|  Java|\n",
      "|    James,,Smith| Scala|\n",
      "|    James,,Smith|   C++|\n",
      "|   Michael,Rose,| Spark|\n",
      "|   Michael,Rose,|  Java|\n",
      "|   Michael,Rose,|   C++|\n",
      "|Robert,,Williams|CSharp|\n",
      "|Robert,,Williams|    VB|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Use explode() function to create a new row for each element in the given array column\"\"\"\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df.select(df[\"name\"],explode(df[\"languagesAtSchool\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8e248740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|  split(name, ,, -1)|\n",
      "+--------------------+\n",
      "|    [James, , Smith]|\n",
      "|   [Michael, Rose, ]|\n",
      "|[Robert, , Williams]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df.select(split(df[\"name\"],\",\")).alias(\"namesArray\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96052404",
   "metadata": {},
   "source": [
    "# array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f71a046e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n",
      "|            name|  States|\n",
      "+----------------+--------+\n",
      "|    James,,Smith|[OH, CA]|\n",
      "|   Michael,Rose,|[NY, NJ]|\n",
      "|Robert,,Williams|[UT, NV]|\n",
      "+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Use array() function to create a new array column by merging the data from multiple columns\"\"\"\n",
    "from pyspark.sql.functions import array\n",
    "\n",
    "df.select(df[\"name\"],array(df[\"currentState\"],df[\"previousState\"]).alias(\"States\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7bea67",
   "metadata": {},
   "source": [
    "# array_contains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "29f1b714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+\n",
      "|            name|array_contains|\n",
      "+----------------+--------------+\n",
      "|    James,,Smith|          true|\n",
      "|   Michael,Rose,|          true|\n",
      "|Robert,,Williams|         false|\n",
      "+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"array_contains() sql function is used to check if array column contains a value\"\"\"\n",
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "df.select(df[\"name\"],array_contains(df[\"languagesAtSchool\"],\"Java\").alias(\"array_contains\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c0700",
   "metadata": {},
   "source": [
    "# MapType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2709e25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"PySpark MapType (also called map type) is a data type to represent Python Dictionary (dict) to store key-value pair\"\"\"\n",
    "from pyspark.sql.types import MapType\n",
    "\n",
    "schema=StructType([\n",
    "    StructField(\"name\",StringType(),True),\n",
    "    StructField(\"properties\",MapType(StringType(),StringType()),True)\n",
    "])\n",
    "\n",
    "dataDictionary = [\n",
    "        ('James',{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',{'hair':'brown','eye':None}),\n",
    "        ('Robert',{'hair':'red','eye':'black'}),\n",
    "        ('Washington',{'hair':'grey','eye':'grey'}),\n",
    "        ('Jefferson',{'hair':'brown','eye':''})\n",
    "        ]\n",
    "\n",
    "df=spark.createDataFrame(data=dataDictionary,schema=schema)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2c4dcb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------+\n",
      "|name      |properties                   |\n",
      "+----------+-----------------------------+\n",
      "|James     |[eye -> brown, hair -> black]|\n",
      "|Michael   |[eye ->, hair -> brown]      |\n",
      "|Robert    |[eye -> black, hair -> red]  |\n",
      "|Washington|[eye -> grey, hair -> grey]  |\n",
      "|Jefferson |[eye -> , hair -> brown]     |\n",
      "+----------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927fc9a2",
   "metadata": {},
   "source": [
    "# PySpark MapType Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "90992979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- hair: string (nullable = true)\n",
      " |-- eye: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3=df.rdd.map(lambda x:(x[\"name\"],x[\"properties\"][\"hair\"],x[\"properties\"][\"eye\"])).toDF([\"name\",\"hair\",\"eye\"])\n",
    "\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7eae3fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n",
      "|      name| hair|  eye|\n",
      "+----------+-----+-----+\n",
      "|     James|black|brown|\n",
      "|   Michael|brown| null|\n",
      "|    Robert|  red|black|\n",
      "|Washington| grey| grey|\n",
      "| Jefferson|brown|     |\n",
      "+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6936b09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n",
      "|      name| hair|  eye|\n",
      "+----------+-----+-----+\n",
      "|     James|black|brown|\n",
      "|   Michael|brown| null|\n",
      "|    Robert|  red|black|\n",
      "|Washington| grey| grey|\n",
      "| Jefferson|brown|     |\n",
      "+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"hair\",df[\"properties\"].getItem(\"hair\"))\\\n",
    ".withColumn(\"eye\",df[\"properties\"].getItem(\"eye\"))\\\n",
    ".drop(\"properties\")\\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7dfd3f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n",
      "|      name| hair|  eye|\n",
      "+----------+-----+-----+\n",
      "|     James|black|brown|\n",
      "|   Michael|brown| null|\n",
      "|    Robert|  red|black|\n",
      "|Washington| grey| grey|\n",
      "| Jefferson|brown|     |\n",
      "+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"hair\",df[\"properties\"][\"hair\"])\\\n",
    ".withColumn(\"eye\",df[\"properties\"][\"eye\"])\\\n",
    ".drop(\"properties\")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "539f8535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+\n",
      "|      name| key|value|\n",
      "+----------+----+-----+\n",
      "|     James| eye|brown|\n",
      "|     James|hair|black|\n",
      "|   Michael| eye| null|\n",
      "|   Michael|hair|brown|\n",
      "|    Robert| eye|black|\n",
      "|    Robert|hair|  red|\n",
      "|Washington| eye| grey|\n",
      "|Washington|hair| grey|\n",
      "| Jefferson| eye|     |\n",
      "| Jefferson|hair|brown|\n",
      "+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name,explode(df.properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259281ee",
   "metadata": {},
   "source": [
    "# map_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "49131708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      name|map_keys(properties)|\n",
      "+----------+--------------------+\n",
      "|     James|         [eye, hair]|\n",
      "|   Michael|         [eye, hair]|\n",
      "|    Robert|         [eye, hair]|\n",
      "|Washington|         [eye, hair]|\n",
      "| Jefferson|         [eye, hair]|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "\n",
    "df.select(df[\"name\"],map_keys(df[\"properties\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "82fea5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+\n",
      "|      name|map_values(properties)|\n",
      "+----------+----------------------+\n",
      "|     James|        [brown, black]|\n",
      "|   Michael|             [, brown]|\n",
      "|    Robert|          [black, red]|\n",
      "|Washington|          [grey, grey]|\n",
      "| Jefferson|             [, brown]|\n",
      "+----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_values\n",
    "\n",
    "df.select(df[\"name\"],map_values(df[\"properties\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd8171",
   "metadata": {},
   "source": [
    "# PySpark Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "dff7cb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) )\n",
    "\n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "\n",
    "df=spark.createDataFrame(data=simpleData,schema=columns)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "38d8039f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a41713",
   "metadata": {},
   "source": [
    "Ranking functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0d4f733c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |2         |\n",
      "|Robert       |Sales     |4100  |3         |\n",
      "|Saif         |Sales     |4100  |4         |\n",
      "|Michael      |Sales     |4600  |5         |\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d39021a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----------+\n",
      "|employee_name|department|salary|        idx|\n",
      "+-------------+----------+------+-----------+\n",
      "|        James|     Sales|  3000|          0|\n",
      "|      Michael|     Sales|  4600|          1|\n",
      "|       Robert|     Sales|  4100| 8589934592|\n",
      "|        Maria|   Finance|  3000| 8589934593|\n",
      "|        James|     Sales|  3000|17179869184|\n",
      "|        Scott|   Finance|  3300|17179869185|\n",
      "|          Jen|   Finance|  3900|25769803776|\n",
      "|         Jeff| Marketing|  3000|25769803777|\n",
      "|        Kumar| Marketing|  2000|25769803778|\n",
      "|         Saif|     Sales|  4100|25769803779|\n",
      "+-------------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df.withColumn(\"idx\", monotonically_increasing_id()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a41b2a",
   "metadata": {},
   "source": [
    "# rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "aac5000e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   3|\n",
      "|         Saif|     Sales|  4100|   3|\n",
      "|      Michael|     Sales|  4600|   5|\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9fc40a",
   "metadata": {},
   "source": [
    "# dense_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "acae1aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense_rank|\n",
      "+-------------+----------+------+----------+\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|       Robert|     Sales|  4100|         2|\n",
      "|         Saif|     Sales|  4100|         2|\n",
      "|      Michael|     Sales|  4600|         3|\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345bf763",
   "metadata": {},
   "source": [
    "# percent_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1565cceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|percent_rank|\n",
      "+-------------+----------+------+------------+\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|       Robert|     Sales|  4100|         0.5|\n",
      "|         Saif|     Sales|  4100|         0.5|\n",
      "|      Michael|     Sales|  4600|         1.0|\n",
      "|        Maria|   Finance|  3000|         0.0|\n",
      "|        Scott|   Finance|  3300|         0.5|\n",
      "|          Jen|   Finance|  3900|         1.0|\n",
      "|        Kumar| Marketing|  2000|         0.0|\n",
      "|         Jeff| Marketing|  3000|         1.0|\n",
      "+-------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    "\n",
    "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594d1fb9",
   "metadata": {},
   "source": [
    "# lag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6bc1b2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary| lag|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|null|\n",
      "|        James|     Sales|  3000|null|\n",
      "|       Robert|     Sales|  4100|3000|\n",
      "|         Saif|     Sales|  4100|3000|\n",
      "|      Michael|     Sales|  4600|4100|\n",
      "|        Maria|   Finance|  3000|null|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|3000|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag\n",
    "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3700e5",
   "metadata": {},
   "source": [
    "# lead()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "effc6e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|lead|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|4100|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|       Robert|     Sales|  4100|4600|\n",
      "|         Saif|     Sales|  4100|null|\n",
      "|      Michael|     Sales|  4600|null|\n",
      "|        Maria|   Finance|  3000|3900|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|null|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead\n",
    "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de86a1",
   "metadata": {},
   "source": [
    "# Window Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2782887b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+---+\n",
      "|department|   avg|  sum| min| max|row|\n",
      "+----------+------+-----+----+----+---+\n",
      "|     Sales|3760.0|18800|3000|4600|  1|\n",
      "|   Finance|3400.0|10200|3000|3900|  1|\n",
      "| Marketing|2500.0| 5000|2000|3000|  1|\n",
      "+----------+------+-----+----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"I will explain how to calculate sum, min, max for each department using PySpark SQL Aggregate window functions and\n",
    "WindowSpec. When working with Aggregate functions, we dont need to use order by clause.\"\"\"\n",
    "\n",
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\",\"row\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc70d2c6",
   "metadata": {},
   "source": [
    "# PySpark SQL Date Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a2bb7503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
    "\n",
    "df=spark.createDataFrame(data,[\"id\",\"date\"])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "47dfb824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|      date|\n",
      "+---+----------+\n",
      "|  1|2020-02-01|\n",
      "|  2|2019-03-01|\n",
      "|  3|2021-03-01|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf60dacb",
   "metadata": {},
   "source": [
    "# current_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fbf3000a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2021-11-17|\n",
      "+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date().alias(\"current_date\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a699023",
   "metadata": {},
   "source": [
    "# date_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e65c2fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|      date|date_format|\n",
      "+----------+-----------+\n",
      "|2020-02-01| 02-01-2020|\n",
      "|2019-03-01| 03-01-2019|\n",
      "|2021-03-01| 03-01-2021|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"date\"),date_format(col(\"date\"),\"MM-dd-yyyy\").alias(\"date_format\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e8ac3",
   "metadata": {},
   "source": [
    "# to_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "48e5fd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|   to_date|\n",
      "+----------+----------+\n",
      "|2020-02-01|2020-02-01|\n",
      "|2019-03-01|2019-03-01|\n",
      "|2021-03-01|2021-03-01|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"date\"),to_date(col(\"date\"),\"yyyy-MM-dd\").alias(\"to_date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b51a00",
   "metadata": {},
   "source": [
    "# datediff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c2031a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|      date|datediff|\n",
      "+----------+--------+\n",
      "|2020-02-01|     655|\n",
      "|2019-03-01|     992|\n",
      "|2021-03-01|     261|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"date\"),datediff(current_date(),col(\"date\")).alias(\"datediff\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806107d8",
   "metadata": {},
   "source": [
    "# months_between()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5a8b7dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      date|months_between_dates|\n",
      "+----------+--------------------+\n",
      "|2020-02-01|         21.51612903|\n",
      "|2019-03-01|         32.51612903|\n",
      "|2021-03-01|          8.51612903|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"date\"),months_between(current_date(),col(\"date\")).alias(\"months_between_dates\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ded7f",
   "metadata": {},
   "source": [
    "# add_months() , date_add(), date_sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d3d73ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+\n",
      "|      date|add_months|sub_months|  date_add|  date_sub|\n",
      "+----------+----------+----------+----------+----------+\n",
      "|2020-02-01|2020-05-01|2019-11-01|2020-02-05|2020-01-28|\n",
      "|2019-03-01|2019-06-01|2018-12-01|2019-03-05|2019-02-25|\n",
      "|2021-03-01|2021-06-01|2020-12-01|2021-03-05|2021-02-25|\n",
      "+----------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#add_months() , date_add(), date_sub()\n",
    "df.select(col(\"date\"), \n",
    "    add_months(col(\"date\"),3).alias(\"add_months\"), \n",
    "    add_months(col(\"date\"),-3).alias(\"sub_months\"), \n",
    "    date_add(col(\"date\"),4).alias(\"date_add\"), \n",
    "    date_sub(col(\"date\"),4).alias(\"date_sub\") \n",
    "  ).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb605172",
   "metadata": {},
   "source": [
    "# year(), month(), month(),next_day(), weekofyear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4add585d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+----------+----------+\n",
      "|      date|year|month|  next_day|weekofyear|\n",
      "+----------+----+-----+----------+----------+\n",
      "|2020-02-01|2020|    2|2020-02-02|         5|\n",
      "|2019-03-01|2019|    3|2019-03-03|         9|\n",
      "|2021-03-01|2021|    3|2021-03-07|         9|\n",
      "+----------+----+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"date\"), \n",
    "     year(col(\"date\")).alias(\"year\"), \n",
    "     month(col(\"date\")).alias(\"month\"), \n",
    "     next_day(col(\"date\"),\"Sunday\").alias(\"next_day\"), \n",
    "     weekofyear(col(\"date\")).alias(\"weekofyear\") \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b4cc11",
   "metadata": {},
   "source": [
    "# dayofweek(), dayofmonth(), dayofyear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "449d5761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---------+\n",
      "|      date|dayofweek|dayofmonth|dayofyear|\n",
      "+----------+---------+----------+---------+\n",
      "|2020-02-01|        7|         1|       32|\n",
      "|2019-03-01|        6|         1|       60|\n",
      "|2021-03-01|        2|         1|       60|\n",
      "+----------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"date\"),  \n",
    "     dayofweek(col(\"date\")).alias(\"dayofweek\"), \n",
    "     dayofmonth(col(\"date\")).alias(\"dayofmonth\"), \n",
    "     dayofyear(col(\"date\")).alias(\"dayofyear\"), \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca1ec2",
   "metadata": {},
   "source": [
    "# current_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c658792f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|id |input                  |\n",
      "+---+-----------------------+\n",
      "|1  |02-01-2020 11 01 19 06 |\n",
      "|2  |03-01-2019 12 01 19 406|\n",
      "|3  |03-01-2021 12 01 19 406|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n",
    "df2=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "872471d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|current_timestamp      |\n",
      "+-----------------------+\n",
      "|2021-11-17 14:23:41.746|\n",
      "+-----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(current_timestamp().alias(\"current_timestamp\")\n",
    "  ).show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f21b606",
   "metadata": {},
   "source": [
    "# to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fa35cf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------+\n",
      "|input                  |to_timestamp           |\n",
      "+-----------------------+-----------------------+\n",
      "|02-01-2020 11 01 19 06 |null                   |\n",
      "|03-01-2019 12 01 19 406|2019-03-01 12:01:19.406|\n",
      "|03-01-2021 12 01 19 406|2021-03-01 12:01:19.406|\n",
      "+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(col(\"input\"), \n",
    "    to_timestamp(col(\"input\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\") \n",
    "  ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "23fb4266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+------+------+\n",
      "|input                  |hour|minute|second|\n",
      "+-----------------------+----+------+------+\n",
      "|2020-02-01 11:01:19.06 |11  |1     |19    |\n",
      "|2019-03-01 12:01:19.406|12  |1     |19    |\n",
      "|2021-03-01 12:01:19.406|12  |1     |19    |\n",
      "+-----------------------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n",
    "df3=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "\n",
    "df3.select(col(\"input\"), \n",
    "    hour(col(\"input\")).alias(\"hour\"), \n",
    "    minute(col(\"input\")).alias(\"minute\"),\n",
    "    second(col(\"input\")).alias(\"second\") \n",
    "  ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac70937",
   "metadata": {},
   "source": [
    "# PySpark JSON Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4dc1ed39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------+\n",
      "|id |value                                                                     |\n",
      "+---+--------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+--------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "jsonString=\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"\n",
    "df=spark.createDataFrame([(1, jsonString)],[\"id\",\"value\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a6611",
   "metadata": {},
   "source": [
    "# from_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e3a3f7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- value: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" from_json() function is used to convert JSON string into Struct type or Map type.\"\"\"\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "df2=df.withColumn(\"value\",from_json(df.value,MapType(StringType(),StringType())))\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2674c98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------------------------------------+\n",
      "|id |value                                                                      |\n",
      "+---+---------------------------------------------------------------------------+\n",
      "|1  |[Zipcode -> 704, ZipCodeType -> STANDARD, City -> PARC PARQUE, State -> PR]|\n",
      "+---+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc314c5",
   "metadata": {},
   "source": [
    "# to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "47bc6386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------------------+\n",
      "|id |value                                                                       |\n",
      "+---+----------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":\"704\",\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"function is used to convert DataFrame columns MapType or Struct type to JSON string\"\"\"\n",
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "df2.withColumn(\"value\",to_json(col(\"value\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2cfb83",
   "metadata": {},
   "source": [
    "# json_tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "7093fa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+-----------+\n",
      "|id |Zipcode|ZipCodeType|City       |\n",
      "+---+-------+-----------+-----------+\n",
      "|1  |704    |STANDARD   |PARC PARQUE|\n",
      "+---+-------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"json_tuple() is used the query or extract the elements from JSON column and create the result as a new columns\"\"\"\n",
    "from pyspark.sql.functions import json_tuple\n",
    "\n",
    "df.select(col(\"id\"),json_tuple(col(\"value\"),\"Zipcode\",\"ZipCodeType\",\"City\")) \\\n",
    "    .toDF(\"id\",\"Zipcode\",\"ZipCodeType\",\"City\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f953e69",
   "metadata": {},
   "source": [
    "# get_json_object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fcc9e01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|id |ZipCodeType|\n",
      "+---+-----------+\n",
      "|1  |STANDARD   |\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object\n",
    "df.select(col(\"id\"),get_json_object(col(\"value\"),\"$.ZipCodeType\").alias(\"ZipCodeType\")) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424dc63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
